---
title: "BMI1 and CHD7 in medulloblastoma primary lines"
subtitle: "Genome-wide DNA methylation analysis"
output: html_notebook
author: "Gabriel Rosser"
---

```{r setup, include=FALSE}
require("knitr")
# opts_knit$set(root.dir = normalizePath('..'))

knitr::opts_chunk$set(echo = TRUE)

require(limma)
require("ChAMP")
require("minfi")
require("wateRmelon")
require("data.table")
require(sva)
require(RColorBrewer)
require(openxlsx)

# this specifies the normalisation algorithm to use
norm.fun <- 'swan'

MVALUE_FN <- sprintf("m_values_1299_3021_%s.xlsx", norm.fun)
META_FN <- "metadata_1299_3021.csv"
```

Check for the required input Excel file. Create it if necessary (from the raw idat files). We should only need to do this once.

```{r echo=FALSE}

if (!file.exists(MVALUE_FN) | !(file.exists(META_FN))) {
  writeLines(
    paste0("The input Excel file ", MVALUE_FN, " was not found. Computing from raw idats.")
  )
  
  source('../_settings.R')
  source("../utils.R")
  
  MfromBeta <- function(beta) {
    log2(beta / (1 - beta))
  }
  
  split_path <- function(x) if (dirname(x)==x) x else c(basename(x),split_path(dirname(x)))
  
  get_idat_basenames <- function(idat.dir) {
    #' Get the basename of all idat files found recursively under the provided directory
    #' The basename is the full path, minus the trailing _Red.idat
    flist <- list.files(path = idat.dir, recursive = T)
    flist <- flist[grep('_Red.idat', flist)]
    
    # convert to basenames for loading
    basenames <- file.path(idat.dir, sub(pattern = "_Red.idat", "", flist))
  }
  
  
  process_idats <- function(
    in.files,
    snames,
    norm.fun=c('swan', 'bmiq', 'funnorm', 'quantile', 'pbc', 'raw'),
    arraytype='EPIC'
  ) {
    norm.fun = match.arg(norm.fun)
    rgSet <- read.metharray(in.files, extended = T)
    colnames(rgSet) <- snames
    
    mset <- preprocessRaw(rgSet)
    detP <- detectionP(rgSet)
    
    # Load beta values (raw), then apply default ChAMP filtering
    beta.raw <- getBeta(mset, "Illumina")
    champLoad <- champ.filter(beta.raw, detP = detP, pd = NULL, arraytype = arraytype)
    beta.raw <- champLoad$beta
    
    if (norm.fun == 'raw') beta <- beta.raw
    
    if (norm.fun == 'swan') {
      mset.swan <- preprocessSWAN(rgSet, mSet = mset)
      beta.swan <- getBeta(mset.swan)
      beta <- beta.swan[rownames(beta.raw),]
    }
    
    if (norm.fun == 'bmiq') {
      beta <- champ.norm(beta = beta.raw, method = 'BMIQ', arraytype = arraytype, cores=4)
    }
    
    if (norm.fun == 'pbc') {
      beta <- champ.norm(beta = beta.raw, method = 'PBC', arraytype = arraytype)
    }
    
    if (norm.fun == 'funnorm') {
      grSet.funnorm <- preprocessFunnorm(rgSet)
      beta <- getBeta(grSet.funnorm)[rownames(beta.raw),]
    }
    
    if (norm.fun == 'quantile') {
      grSet.quantile <- preprocessQuantile(rgSet)
      beta <- getBeta(grSet.quantile)[rownames(beta.raw),]
    }
    
    return(list(beta.raw=beta.raw, beta=beta))
    
  }

# load data from raw

failed_filenames <- c(
  "202081130238/202081130238_R01C01"
)
samples <- c(
  'ICb1299_Scr', 
  'ICb1299_shBMI1', 
  'ICb1299_shCHD7', 
  'ICb1299_shBMI1CHD7', 
  'p62_3_shBmi1', 
  'p62_3_shChd7', 
  'p62_3_shB+C',
  'p62_3_Scr', 
  '3021_1_Scr', 
  '3021_1_shB', 
  '3021_1_shC', 
  '3021_1_shB+C',
  'S', 
  'B', 
  'C', 
  'B+C'
)

base.dirs <- c(
  file.path(data.dir.raid, 'methylation', '2017-09-19'),
  file.path(data.dir.raid, 'methylation', '2018-01-12'),
  file.path(data.dir.raid, 'methylation', '2018-03-19'),
  file.path(data.dir.raid, 'methylation', '2018-03-26'),
  file.path(data.dir.raid, 'methylation', '2018-04-09')
)

in.files <- NULL
snames <- NULL
batches <- NULL

for (b in base.dirs) {
  meta <- read.csv(file.path(b, 'sources.csv'))
  # set the rownames as filenames
  rownames(meta) <- paste(meta$Sentrix_ID, meta$Sentrix_Position, sep = '_')
  this_files <- get_idat_basenames(file.path(b, 'idat'))
  
  # reorder meta
  meta <- meta[basename(this_files),]
  
  # filter meta
  idx <- meta[, 'sample'] %in% samples
  
  # define file and sample names and add to list
  this_files <- this_files[idx]
  this_snames <- as.vector(meta[idx, 'sample'])
  this_batches <- as.vector(sapply(this_files, function(x){split_path(x)[4]}))
  
  in.files <- c(in.files, this_files)
  snames <- c(snames, this_snames)
  batches <- c(batches, this_batches)
}

# manually remove a failed sample
inds <- unlist(lapply(failed_filenames, function(x) grep(x, in.files)))
if (!is.null(inds)) {
  in.files <- in.files[-inds]
  snames <- snames[-inds]
  batches <- batches[-inds]
}

res <- process_idats(in.files, snames, norm.fun=norm.fun)
beta <- res$beta
beta.raw <- res$beta.raw

# simple metadata
meta <- data.frame(row.names = snames)

meta$cell_line <- '3021'
meta[grep('1299', rownames(meta)), 'cell_line'] <- '1299'
meta[grep('p62', rownames(meta)), 'cell_line'] <- '1299'

meta$batch <- batches

meta$condition <- 'Scr'
meta[grep('bmi1', rownames(meta), ignore.case = T), 'condition'] <- 'shBMI1'
meta[grep('shb', rownames(meta), ignore.case = T), 'condition'] <- 'shBMI1'

meta[grep('chd', rownames(meta), ignore.case = T), 'condition'] <- 'shCHD7'
meta[grep('shc', rownames(meta), ignore.case = T), 'condition'] <- 'shCHD7'

meta[grep('\\+', rownames(meta)), 'condition'] <- 'shBMI1shCHD7'
meta[grep('bmi1chd7', rownames(meta), ignore.case = T), 'condition'] <- 'shBMI1shCHD7'
meta[rownames(meta) == 'B', 'condition'] <- 'shBMI1'
meta[rownames(meta) == 'C', 'condition'] <- 'shCHD7'

m <- MfromBeta(beta)

# save M data to Excel file.
write.xlsx(m, MVALUE_FN, colNames = T, rowNames = T)
# save metadata to CSV file.
write.csv(meta, META_FN)

} else {
  
  BetafromM <- function(m) {
    (2 ** m) / (1 + 2 ** m)
  }
  
  writeLines(
    paste0("Loading pre-computed M data from ", MVALUE_FN),
    paste0("Loading pre-computed metadata from ", META_FN)
  )
  m <- read.xlsx(MVALUE_FN, rowNames = T)
  beta <- BetafromM(m)
  meta <- read.csv(META_FN, row.names = 1)
}
```

Run a few exploratory checks.

```{r}

densityPlot(beta, sampGroups = meta$batch, ylim = c(0,7))
densityPlot(beta, sampGroups = meta$cell_line, ylim = c(0,7))

pal <- brewer.pal(8,"Dark2")
plotMDS(m, top=NULL, col=pal[factor(meta$batch)])
legend("topright", legend=levels(factor(meta$batch)), text.col=pal, bg="white")

plotMDS(m[, meta$cell_line == '1299'], top=NULL, col=pal[factor(meta[meta$cell_line == '1299', 'batch'])])
legend("topright", legend=levels(factor(meta[meta$cell_line == '1299', 'batch'])), text.col=pal, bg="white")

ix <- meta$cell_line == '3021'
plotMDS(m[, ix], top=NULL, col=pal[factor(meta[ix, 'batch'])])
legend("topright", legend=levels(factor(meta[ix, 'batch'])), text.col=pal, bg="white")

```

Now let's identify differentially methylated probes (DMPs). We'll carry out a separate analysis for each of the two cell lines (1299 and 3021). 

Before searching for DMPs, we'll run two filtering steps:

1. Filtering based on the range of M values and requiring a minimum variation across the samples.
2. Requiring consistency within each of the experimental groups, such that the variation in M value cannot exceed a threshold.

If we don't do this, our p values will all come out non-significant following the correction for multiple hypothesis testing.

```{r}
minimum_range <- 1.
maximum_variation_within_group <- 5.
# this is the FDR cutoff, only used for reporting purposes (all DMPs with unadjusted P < 0.05 are exported)
alpha <- 0.05

dmps <- list()
dmps.sv <- list()

for (cl in c('3021', '1299')) {
  dmps[[cl]] <- list()
  
  writeLines(paste0("Cell line ", cl))
  this.meta <- droplevels(meta[meta$cell_line == cl,])
  this.m <- m[, rownames(this.meta)]
  
  # Filter 1: minimum range
  
  n_before <- nrow(this.m)
  rg <- apply(this.m, 1, range)
  rg <- rg[2,] - rg[1,]
  ix <- rg > minimum_range
  
  hist(rg, 40, xlab='M value range', main=paste0(cl, ' M value range'))
  abline(v=minimum_range, col='red', lty=2)
  
  this.m <- this.m[ix,]
  n_after <- nrow(this.m)
  writeLines(
    sprintf(
      "Filtering based on a minimum range threshold of %.2f results in the number of probes going from %d to %d (removing %d)",
      minimum_range,
      n_before,
      n_after,
      n_before - n_after
    )
  )
  
  condition <- factor(this.meta$condition)
  batch <- make.names(factor(this.meta$batch))
  
  # Filter 2: Maximum variation
  # this should be possible to do efficiently using group_by from dplyr, but I hate R and it hates me.
  group_probes <- list()
  for (cnd in levels(condition)) {
    n_before <- nrow(this.m)
    this.group.m <- this.m[,condition == cnd]
    rg <- apply(this.group.m, 1, range)
    rg <- rg[2,] - rg[1,]
    ix <- rg < maximum_variation_within_group
    this.group.m <- this.group.m[ix,]
    n_after <- nrow(this.group.m)
    writeLines(
      sprintf(
        "Filtering based on a maximum inter-group threshold of %.2f results in the number of probes in group %s going from %d to %d (removing %d)",
        maximum_variation_within_group,
        cnd,
        n_before,
        n_after,
        n_before - n_after
      )
    )
    group_probes[[cnd]] <- rownames(this.group.m)
  }
  
  # now eliminate any probes that were removed
  common_probes = Reduce(intersect, group_probes)
  n_before <- nrow(this.m)
  this.m <- this.m[common_probes,]
  n_after <- nrow(this.m)
  writeLines(
    sprintf(
      "After the second stage of filtering (inter-group variation), we reduced the number of probes from %d to %d (removed %d)",
      n_before,
      n_after,
      n_before - n_after
    )
  )

  # run DMP WITHOUT batch correction
  
  design <- model.matrix(~0 + condition + batch)
  fit <- lmFit(this.m, design)
  contrasts <- makeContrasts(
    conditionshBMI1-conditionScr, 
    conditionshCHD7-conditionScr, 
    conditionshBMI1shCHD7-conditionshBMI1, 
    conditionshBMI1shCHD7-conditionshCHD7, 
    conditionshBMI1shCHD7-conditionScr, 
    levels=design
  )

  fit2 <- contrasts.fit(fit, contrasts)
  fit2 <- eBayes(fit2, trend = T, robust = T)
  
  for (i in seq(ncol(contrasts))) {
    ttl <- colnames(contrasts)[i]
    ttl <- gsub(pattern = "condition", replacement = '', x = ttl)
    this_res <- topTable(fit2, coef = i, number = Inf)
    this_res <- this_res[this_res$P.Value < 0.05,]
    dmps[[cl]][[ttl]] <- this_res
    writeLines(sprintf(
      "Comparison %s: %d DMPs with FDR < %.2f",
      ttl,
      sum(this_res$adj.P.Val < alpha),
      alpha
    ))
  }
  out_fn <- paste0("dmps_", cl, ".xlsx")
  write.xlsx(dmps[[cl]], out_fn, colNames = T, rowNames = T)
  writeLines(
    sprintf(
      "Wrote the DMP results to file %s", 
      out_fn
    )
  )
  
  # run DMP WITH batch correction
  dmps.sv[[cl]] <- list()
  
  mod <- model.matrix(~0+condition+batch, data=this.meta)
  mod_null <- model.matrix(~0+batch, data=this.meta)
  # get SVA to estimate the number of latent variables
  n.sv <- num.sv(as.matrix(this.m), mod, method='leek')
  svobj <- sva(as.matrix(this.m), mod, mod_null, n.sv=n.sv)
  
  fit <- lmFit(this_data, mod.Sv)
  contrasts <- makeContrasts(
    conditionshBMI1-conditionScr, 
    conditionshCHD7-conditionScr, 
    conditionshBMI1shCHD7-conditionshBMI1, 
    conditionshBMI1shCHD7-conditionshCHD7, 
    conditionshBMI1shCHD7-conditionScr, 
    levels=mod.Sv
  )

  fit2 <- contrasts.fit(fit, contrasts)
  fit2 <- eBayes(fit2, trend = T, robust=T)
  
  for (i in seq(ncol(contrasts))) {
    ttl <- colnames(contrasts)[i]
    ttl <- gsub(pattern = "condition", replacement = '', x = ttl)
    this_res <- topTable(fit2, coef = i, number = Inf)
    this_res <- this_res[this_res$P.Value < 0.05,]
    dmps.sv[[cl]][[ttl]] <- this_res
    writeLines(sprintf(
      "BATCH CORRECTED Comparison %s: %d DMPs with FDR < %.2f",
      ttl,
      sum(this_res$adj.P.Val < alpha),
      alpha
    ))
  }
  out_fn <- paste0("dmps_sva_", cl, ".xlsx")
  write.xlsx(dmps.sv[[cl]], out_fn, colNames = T, rowNames = T)
  writeLines(
    sprintf(
      "Wrote the DMP results to file %s", 
      out_fn
    )
  )

}

```


Now let's generate the volcano plots.

```{r}
for (cl in c('3021', '1299')) {
  this.dmps <- dmps[[cl]]
  the_names <- names(this.dmps)
  for (cmp in the_names) {
    ttl <- paste(cl, cmp, sep = ' ')
    x <- this.dmps[[cmp]]
    absx_max <- ceiling(max(abs(x$logFC)))
    plot(x$logFC, -log10(x$adj.P.Val), main=ttl, xlim=c(-absx_max, absx_max))
  }
}

```
The 3021 line results seem to show evidence of dependence (i.e. a functional link between log fold change and adjusted P value). Here are a few forum posts discussing this:

- https://support.bioconductor.org/p/111776/
- https://support.bioconductor.org/p/95197/
- https://support.bioconductor.org/p/57445/

In summary: the estimate of prior degrees of freedom is inifinity, meaning that all genes end up with the same estimated dispersion, hence P value is a function of log fold change. We can (partially) fix this by ensuring we run `eBayes()` with `robust=T` and `trend=T`.

